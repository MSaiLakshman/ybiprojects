{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataScienceJobs.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMFsTHmzhYfcbAP1GBXaptR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lhr8RbXkI3NY"},"outputs":[],"source":["importing Libraries\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","plt.style.use('fivethirtyeight')\n","import seaborn as sns\n","import warnings \n","warnings.filterwarnings(action='ignore')\n","train=pd.read_csv(\"../input/hr-analytics-job-change-of-data-scientists/aug_train.csv\")\n","test=pd.read_csv(\"../input/hr-analytics-job-change-of-data-scientists/aug_test.csv\")\n","train.head()\n","enrollee_id\tcity\tcity_development_index\tgender\trelevent_experience\tenrolled_university\teducation_level\tmajor_discipline\texperience\tcompany_size\tcompany_type\tlast_new_job\ttraining_hours\ttarget\n","0\t8949\tcity_103\t0.920\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t>20\tNaN\tNaN\t1\t36\t1.0\n","1\t29725\tcity_40\t0.776\tMale\tNo relevent experience\tno_enrollment\tGraduate\tSTEM\t15\t50-99\tPvt Ltd\t>4\t47\t0.0\n","2\t11561\tcity_21\t0.624\tNaN\tNo relevent experience\tFull time course\tGraduate\tSTEM\t5\tNaN\tNaN\tnever\t83\t0.0\n","3\t33241\tcity_115\t0.789\tNaN\tNo relevent experience\tNaN\tGraduate\tBusiness Degree\t<1\tNaN\tPvt Ltd\tnever\t52\t1.0\n","4\t666\tcity_162\t0.767\tMale\tHas relevent experience\tno_enrollment\tMasters\tSTEM\t>20\t50-99\tFunded Startup\t4\t8\t0.0\n","Data Cleaning\n","Let's clean the data first...\n","\n","train.isnull().sum()\n","enrollee_id                  0\n","city                         0\n","city_development_index       0\n","gender                    4508\n","relevent_experience          0\n","enrolled_university        386\n","education_level            460\n","major_discipline          2813\n","experience                  65\n","company_size              5938\n","company_type              6140\n","last_new_job               423\n","training_hours               0\n","target                       0\n","dtype: int64\n","There are a lot of null values in this dataset so we will first check the shape if we can directly remove all the rows with empty values and than check if it is sufficient to predict the test data...\n","\n","train.shape\n","(19158, 14)\n","So we will use dropna...\n","\n","train=train.dropna()\n","After dropping the rows we will reset index...\n","\n","train=train.reset_index(drop=True)\n","train.shape\n","(8955, 14)\n","Now, we will check test dataset if it can be predicted with the number of values in train dataset...\n","\n","test.shape\n","(2129, 13)\n","Now we will check the null values in test dataset...\n","\n","test.isnull().sum()\n","enrollee_id                 0\n","city                        0\n","city_development_index      0\n","gender                    508\n","relevent_experience         0\n","enrolled_university        31\n","education_level            52\n","major_discipline          312\n","experience                  5\n","company_size              622\n","company_type              634\n","last_new_job               40\n","training_hours              0\n","dtype: int64\n","We cannot drop the null values in test dataset as the submission file has the same number of values to be predicted. So, we will have to fill the empty values in test dataset...\n","\n","test.head()\n","enrollee_id\tcity\tcity_development_index\tgender\trelevent_experience\tenrolled_university\teducation_level\tmajor_discipline\texperience\tcompany_size\tcompany_type\tlast_new_job\ttraining_hours\n","0\t32403\tcity_41\t0.827\tMale\tHas relevent experience\tFull time course\tGraduate\tSTEM\t9\t<10\tNaN\t1\t21\n","1\t9858\tcity_103\t0.920\tFemale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t5\tNaN\tPvt Ltd\t1\t98\n","2\t31806\tcity_21\t0.624\tMale\tNo relevent experience\tno_enrollment\tHigh School\tNaN\t<1\tNaN\tPvt Ltd\tnever\t15\n","3\t27385\tcity_13\t0.827\tMale\tHas relevent experience\tno_enrollment\tMasters\tSTEM\t11\t10/49\tPvt Ltd\t1\t39\n","4\t27724\tcity_103\t0.920\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t>20\t10000+\tPvt Ltd\t>4\t72\n","#Filling gender column \n","\n","print(test['gender'].mode())\n","0    Male\n","dtype: object\n","test['gender'].fillna(value='Male',inplace=True)\n","#Filling enrolled_university column\n","\n","test['enrolled_university'].mode()\n","0    no_enrollment\n","dtype: object\n","test['enrolled_university'].fillna(value='no_enrollment',inplace=True)\n","#Filling education_level\n","\n","test['education_level'].mode()\n","0    Graduate\n","dtype: object\n","test['education_level'].fillna(value='Graduate',inplace=True)\n","#Filling major_discipline \n","\n","test['major_discipline'].mode()\n","0    STEM\n","dtype: object\n","test['major_discipline'].fillna(value='STEM',inplace=True)\n","#Filling experience\n","\n","test['experience'].mode()\n","0    >20\n","dtype: object\n","test['experience'].fillna(value='>20',inplace=True)\n","#Filling company size\n","\n","test['company_size'].mode()\n","0    50-99\n","dtype: object\n","test['company_size'].fillna(value='50-99',inplace=True)\n","#Filling company_type\n","\n","test['company_type'].mode()\n","0    Pvt Ltd\n","dtype: object\n","test['company_type'].fillna(value='Pvt Ltd',inplace=True)\n","#Filling last new job\n","\n","test['last_new_job'].mode()\n","0    1\n","dtype: object\n","test['last_new_job'].fillna(value='1',inplace=True)\n","test.isnull().sum()\n","enrollee_id               0\n","city                      0\n","city_development_index    0\n","gender                    0\n","relevent_experience       0\n","enrolled_university       0\n","education_level           0\n","major_discipline          0\n","experience                0\n","company_size              0\n","company_type              0\n","last_new_job              0\n","training_hours            0\n","dtype: int64\n","So now no values are empty in train and test dataset...\n","\n","Data Visualization\n","train.head()\n","enrollee_id\tcity\tcity_development_index\tgender\trelevent_experience\tenrolled_university\teducation_level\tmajor_discipline\texperience\tcompany_size\tcompany_type\tlast_new_job\ttraining_hours\ttarget\n","0\t29725\tcity_40\t0.776\tMale\tNo relevent experience\tno_enrollment\tGraduate\tSTEM\t15\t50-99\tPvt Ltd\t>4\t47\t0.0\n","1\t666\tcity_162\t0.767\tMale\tHas relevent experience\tno_enrollment\tMasters\tSTEM\t>20\t50-99\tFunded Startup\t4\t8\t0.0\n","2\t402\tcity_46\t0.762\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t13\t<10\tPvt Ltd\t>4\t18\t1.0\n","3\t27107\tcity_103\t0.920\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t7\t50-99\tPvt Ltd\t1\t46\t1.0\n","4\t23853\tcity_103\t0.920\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t5\t5000-9999\tPvt Ltd\t1\t108\t0.0\n","plt.figure(figsize=(12,6))\n","sns.countplot(train['gender'])\n","plt.show()\n","\n","index=train['relevent_experience'].value_counts().index\n","values=train['relevent_experience'].value_counts()\n","plt.figure(figsize=(16,8))\n","plt.pie(values,labels=index,autopct=\"%1.1f%%\",wedgeprops={'edgecolor':'black'})\n","plt.show()\n","\n","index=train['enrolled_university'].value_counts().index\n","values=train['enrolled_university'].value_counts()\n","plt.figure(figsize=(12,6))\n","plt.bar(index, values, color ='maroon')\n","plt.xlabel(\"enrolled_university\")\n","plt.ylabel(\"count\")\n","plt.show()\n","\n","index=train['education_level'].value_counts().index\n","values=train['education_level'].value_counts()\n","plt.figure(figsize=(16,8))\n","plt.pie(values,labels=index,autopct=\"%1.1f%%\",wedgeprops={'edgecolor':'black'},explode=[0.1,0,0])\n","plt.show()\n","\n","plt.figure(figsize=(12,6))\n","sns.countplot(train['major_discipline'])\n","plt.show()\n","print(train['major_discipline'].value_counts())\n","\n","STEM               7989\n","Humanities          378\n","Other               177\n","Business Degree     170\n","Arts                129\n","No Major            112\n","Name: major_discipline, dtype: int64\n","Here STEM stands for Science, technology, engineering and mathematics...\n","\n","index=train['company_size'].value_counts().index\n","values=train['company_size'].value_counts()\n","plt.figure(figsize=(18,10))\n","plt.xlabel('company_size')\n","plt.pie(values,labels=index,autopct=\"%1.1f%%\",wedgeprops={\"edgecolor\":\"black\"},explode=[0,0,0.1,0,0,0,0,0])\n","plt.show()\n","\n","index=train['company_type'].value_counts().index\n","values=train['company_type'].value_counts()\n","plt.figure(figsize=(12,6))\n","plt.bar(index, values, color ='#01153E')\n","plt.xlabel(\"company_type\")\n","plt.ylabel(\"count\")\n","plt.show()\n","\n","plt.figure(figsize=(12,6))\n","sns.distplot(train['training_hours'],color='maroon')\n","plt.show()\n","\n","train.head()\n","enrollee_id\tcity\tcity_development_index\tgender\trelevent_experience\tenrolled_university\teducation_level\tmajor_discipline\texperience\tcompany_size\tcompany_type\tlast_new_job\ttraining_hours\ttarget\n","0\t29725\tcity_40\t0.776\tMale\tNo relevent experience\tno_enrollment\tGraduate\tSTEM\t15\t50-99\tPvt Ltd\t>4\t47\t0.0\n","1\t666\tcity_162\t0.767\tMale\tHas relevent experience\tno_enrollment\tMasters\tSTEM\t>20\t50-99\tFunded Startup\t4\t8\t0.0\n","2\t402\tcity_46\t0.762\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t13\t<10\tPvt Ltd\t>4\t18\t1.0\n","3\t27107\tcity_103\t0.920\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t7\t50-99\tPvt Ltd\t1\t46\t1.0\n","4\t23853\tcity_103\t0.920\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t5\t5000-9999\tPvt Ltd\t1\t108\t0.0\n","train['experience'].value_counts()\n",">20    1881\n","5       575\n","10      551\n","6       535\n","9       533\n","7       485\n","4       480\n","3       428\n","8       398\n","15      398\n","11      372\n","14      333\n","16      304\n","2       297\n","12      293\n","13      235\n","17      206\n","19      179\n","18      168\n","1       121\n","<1       97\n","20       86\n","Name: experience, dtype: int64\n","Only >20 years experience has a large number of values than other. So, there is no need to compare them on that basis as it will be in appropriate. So, we will use data of only exp>20...\n","\n","exp_20=train.loc[train['experience']=='>20']\n","exp_20.head()\n","enrollee_id\tcity\tcity_development_index\tgender\trelevent_experience\tenrolled_university\teducation_level\tmajor_discipline\texperience\tcompany_size\tcompany_type\tlast_new_job\ttraining_hours\ttarget\n","1\t666\tcity_162\t0.767\tMale\tHas relevent experience\tno_enrollment\tMasters\tSTEM\t>20\t50-99\tFunded Startup\t4\t8\t0.0\n","5\t25619\tcity_61\t0.913\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t>20\t1000-4999\tPvt Ltd\t3\t23\t0.0\n","11\t14928\tcity_103\t0.920\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t>20\t100-500\tPvt Ltd\t3\t40\t0.0\n","13\t26966\tcity_160\t0.920\tFemale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t>20\t100-500\tPvt Ltd\t>4\t82\t0.0\n","16\t10164\tcity_114\t0.926\tMale\tHas relevent experience\tno_enrollment\tPhd\tSTEM\t>20\t100-500\tPvt Ltd\t4\t42\t1.0\n","plt.figure(figsize=(12,6))\n","sns.distplot(exp_20['training_hours'],color='maroon')\n","plt.show()\n","\n","plt.figure(figsize=(12,6))\n","sns.countplot(exp_20['company_size'])\n","plt.show()\n","\n","We can see that people who have experience greater than 20 are even highest in a small company_size and we can also see that more are in company_size in 10000+ which is appropriate according to the condition...\n","\n","plt.figure(figsize=(12,6))\n","sns.countplot(exp_20['company_type'])\n","plt.show()\n","\n","train.head()\n","enrollee_id\tcity\tcity_development_index\tgender\trelevent_experience\tenrolled_university\teducation_level\tmajor_discipline\texperience\tcompany_size\tcompany_type\tlast_new_job\ttraining_hours\ttarget\n","0\t29725\tcity_40\t0.776\tMale\tNo relevent experience\tno_enrollment\tGraduate\tSTEM\t15\t50-99\tPvt Ltd\t>4\t47\t0.0\n","1\t666\tcity_162\t0.767\tMale\tHas relevent experience\tno_enrollment\tMasters\tSTEM\t>20\t50-99\tFunded Startup\t4\t8\t0.0\n","2\t402\tcity_46\t0.762\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t13\t<10\tPvt Ltd\t>4\t18\t1.0\n","3\t27107\tcity_103\t0.920\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t7\t50-99\tPvt Ltd\t1\t46\t1.0\n","4\t23853\tcity_103\t0.920\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t5\t5000-9999\tPvt Ltd\t1\t108\t0.0\n","Changing target variable to integer...\n","\n","train['target']=train['target'].astype('int64')\n","Comparing features with target...\n","0-Not looking for job change , 1-Looking for job change\n","\n","plt.figure(figsize=(12,6))\n","sns.countplot(x=\"gender\",hue=\"target\",data=train,palette=['#3D1C02',\"#AAA602\"])\n","plt.show()\n","\n","plt.figure(figsize=(12,6))\n","sns.countplot(x=\"relevent_experience\",hue=\"target\",data=train,palette=['#8C000F',\"#F0E68C\"])\n","plt.show()\n","\n","plt.figure(figsize=(12,6))\n","sns.countplot(x=\"experience\",hue=\"target\",data=train,palette=['#432371',\"#FAAE7B\"])\n","plt.show()\n","\n","plt.figure(figsize=(12,6))\n","sns.countplot(x=\"company_size\",hue=\"target\",data=train,palette=['#054907',\"#D2691E\"])\n","plt.show()\n","\n","plt.figure(figsize=(12,6))\n","sns.countplot(x=\"company_type\",hue=\"target\",data=train,palette=['#580F41',\"#FFC0CB\"])\n","plt.show()\n","\n","plt.figure(figsize=(12,6))\n","sns.countplot(x=\"last_new_job\",hue=\"target\",data=train,palette=['#6E750E',\"#E6DAA6\"])\n","plt.show()\n","\n","train.head()\n","enrollee_id\tcity\tcity_development_index\tgender\trelevent_experience\tenrolled_university\teducation_level\tmajor_discipline\texperience\tcompany_size\tcompany_type\tlast_new_job\ttraining_hours\ttarget\n","0\t29725\tcity_40\t0.776\tMale\tNo relevent experience\tno_enrollment\tGraduate\tSTEM\t15\t50-99\tPvt Ltd\t>4\t47\t0\n","1\t666\tcity_162\t0.767\tMale\tHas relevent experience\tno_enrollment\tMasters\tSTEM\t>20\t50-99\tFunded Startup\t4\t8\t0\n","2\t402\tcity_46\t0.762\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t13\t<10\tPvt Ltd\t>4\t18\t1\n","3\t27107\tcity_103\t0.920\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t7\t50-99\tPvt Ltd\t1\t46\t1\n","4\t23853\tcity_103\t0.920\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t5\t5000-9999\tPvt Ltd\t1\t108\t0\n","train['relevent_experience']=train['relevent_experience'].replace({'No relevent experience':0,'Has relevent experience':1})\n","Now we will use get_dummies for those columns who have more than 2 categorical values as it works more accurately when we predict our values as our target is in 2 categorical values manner...\n","\n","dummies=pd.get_dummies(train[['enrolled_university']])\n","train = pd.concat([train.drop(['enrolled_university'],axis=1), dummies],axis=1)\n","dummies=pd.get_dummies(train[['gender']])\n","train = pd.concat([train.drop(['gender'],axis=1), dummies],axis=1)\n","I am dropping here education level as when i tested it with get_dummies than one value was making more column in test dataset as that value droppped from train in dropna, thus increasing the columns in test dataset...\n","\n","train=train.drop(\"education_level\",axis=1)\n","dummies=pd.get_dummies(train[['major_discipline']])\n","train = pd.concat([train.drop(['major_discipline'],axis=1), dummies],axis=1)\n","dummies=pd.get_dummies(train[['experience']])\n","train = pd.concat([train.drop(['experience'],axis=1), dummies],axis=1)\n","dummies=pd.get_dummies(train[['company_size']])\n","train = pd.concat([train.drop(['company_size'],axis=1), dummies],axis=1)\n","dummies=pd.get_dummies(train[['company_type']])\n","train = pd.concat([train.drop(['company_type'],axis=1), dummies],axis=1)\n","dummies=pd.get_dummies(train[['last_new_job']])\n","train = pd.concat([train.drop(['last_new_job'],axis=1), dummies],axis=1)\n","Similarly in test dataset\n","\n","test.head()\n","enrollee_id\tcity\tcity_development_index\tgender\trelevent_experience\tenrolled_university\teducation_level\tmajor_discipline\texperience\tcompany_size\tcompany_type\tlast_new_job\ttraining_hours\n","0\t32403\tcity_41\t0.827\tMale\tHas relevent experience\tFull time course\tGraduate\tSTEM\t9\t<10\tPvt Ltd\t1\t21\n","1\t9858\tcity_103\t0.920\tFemale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t5\t50-99\tPvt Ltd\t1\t98\n","2\t31806\tcity_21\t0.624\tMale\tNo relevent experience\tno_enrollment\tHigh School\tSTEM\t<1\t50-99\tPvt Ltd\tnever\t15\n","3\t27385\tcity_13\t0.827\tMale\tHas relevent experience\tno_enrollment\tMasters\tSTEM\t11\t10/49\tPvt Ltd\t1\t39\n","4\t27724\tcity_103\t0.920\tMale\tHas relevent experience\tno_enrollment\tGraduate\tSTEM\t>20\t10000+\tPvt Ltd\t>4\t72\n","test['relevent_experience']=test['relevent_experience'].replace({'No relevent experience':0,'Has relevent experience':1})\n","dummies=pd.get_dummies(test[['gender']])\n","test = pd.concat([test.drop(['gender'],axis=1), dummies],axis=1)\n","dummies=pd.get_dummies(test[['enrolled_university']])\n","test = pd.concat([test.drop(['enrolled_university'],axis=1), dummies],axis=1)\n","test=test.drop(\"education_level\",axis=1)\n","dummies=pd.get_dummies(test[['major_discipline']])\n","test = pd.concat([test.drop(['major_discipline'],axis=1), dummies],axis=1)\n","dummies=pd.get_dummies(test[['experience']])\n","test = pd.concat([test.drop(['experience'],axis=1), dummies],axis=1)\n","dummies=pd.get_dummies(test[['company_size']])\n","test = pd.concat([test.drop(['company_size'],axis=1), dummies],axis=1)\n","dummies=pd.get_dummies(test[['company_type']])\n","test = pd.concat([test.drop(['company_type'],axis=1), dummies],axis=1)\n","dummies=pd.get_dummies(test[['last_new_job']])\n","test = pd.concat([test.drop(['last_new_job'],axis=1), dummies],axis=1)\n","print(train.shape)\n","print(test.shape)\n","(8955, 60)\n","(2129, 59)\n","So we have done with this part as we can see that there is one more column present in train dataset which is target variable...\n","\n","Machine Learning\n","train.columns\n","Index(['enrollee_id', 'city', 'city_development_index', 'relevent_experience',\n","       'training_hours', 'target', 'enrolled_university_Full time course',\n","       'enrolled_university_Part time course',\n","       'enrolled_university_no_enrollment', 'gender_Female', 'gender_Male',\n","       'gender_Other', 'major_discipline_Arts',\n","       'major_discipline_Business Degree', 'major_discipline_Humanities',\n","       'major_discipline_No Major', 'major_discipline_Other',\n","       'major_discipline_STEM', 'experience_1', 'experience_10',\n","       'experience_11', 'experience_12', 'experience_13', 'experience_14',\n","       'experience_15', 'experience_16', 'experience_17', 'experience_18',\n","       'experience_19', 'experience_2', 'experience_20', 'experience_3',\n","       'experience_4', 'experience_5', 'experience_6', 'experience_7',\n","       'experience_8', 'experience_9', 'experience_<1', 'experience_>20',\n","       'company_size_10/49', 'company_size_100-500', 'company_size_1000-4999',\n","       'company_size_10000+', 'company_size_50-99', 'company_size_500-999',\n","       'company_size_5000-9999', 'company_size_<10',\n","       'company_type_Early Stage Startup', 'company_type_Funded Startup',\n","       'company_type_NGO', 'company_type_Other', 'company_type_Public Sector',\n","       'company_type_Pvt Ltd', 'last_new_job_1', 'last_new_job_2',\n","       'last_new_job_3', 'last_new_job_4', 'last_new_job_>4',\n","       'last_new_job_never'],\n","      dtype='object')\n","We will use .columns function as we have increased a lot of columns and will drop columns directly from the X which are not required...\n","\n","TRAIN TEST SPLITTING\n","\n","X=train.drop(['enrollee_id','city','target'],axis=1)\n","y=train[['target']]\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)\n","(6268, 57)\n","(6268, 1)\n","(2687, 57)\n","(2687, 1)\n","Logistic Regression\n","from sklearn.linear_model import LogisticRegression\n","model=LogisticRegression()\n","model.fit(X_train,y_train)\n","LogisticRegression()\n","predictions=model.predict(X_test)\n","model.score(X_test,y_test)\n","0.8395980647562337\n","Decision Tree Classifier\n","from sklearn.tree import DecisionTreeClassifier\n","model_2=DecisionTreeClassifier()\n","model_2.fit(X_train,y_train)\n","DecisionTreeClassifier()\n","predictions_2=model_2.predict(X_test)\n","model_2.score(X_test,y_test)\n","0.777074804614812\n","Random Forest Classifier\n","from sklearn.ensemble import RandomForestClassifier\n","model_3=RandomForestClassifier()\n","model_3.fit(X_train,y_train)\n","RandomForestClassifier()\n","predictions_3=model_3.predict(X_test)\n","model_3.score(X_test,y_test)\n","0.8474134722739114\n","KNeighbors Classifier\n","from sklearn.neighbors import KNeighborsClassifier\n","model_4=KNeighborsClassifier()\n","model_4.fit(X_train,y_train)\n","KNeighborsClassifier()\n","predictions_4=model_4.predict(X_test)\n","model_4.score(X_test,y_test)\n","0.8228507629326386\n","SVM\n","from sklearn.svm import SVC\n","model_5=SVC()\n","model_5.fit(X_train,y_train)\n","SVC()\n","predictions_5=model_5.predict(X_test)\n","model_5.score(X_test,y_test)\n","0.8373650911797543\n","So now maximum score in prediction is of Random Forest Classifier. So now we will predict our target variable by using this algorithm...\n","\n","test=test.drop(['enrollee_id','city'],axis=1)\n","model_3.fit(X,y)\n","RandomForestClassifier()\n","final_predictions=model_3.predict(test)\n","final_predictions\n","array([0, 0, 0, ..., 0, 0, 0])\n","with np.printoptions(threshold=np.inf):\n","    print(final_predictions)\n","[0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n"," 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n"," 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n"," 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n"," 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0\n"," 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n"," 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n"," 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0\n"," 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n"," 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]"]}]}