{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CreditcardFraudDetection.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOlu8WFn5GWVQ57FP1kHUOf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VNek6fH7CMCi"},"outputs":[],"source":["Importing necessary Libraries\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from sklearn import metrics\n","from sklearn.metrics import make_scorer,f1_score,r2_score\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.metrics import roc_auc_score, log_loss, accuracy_score,roc_curve\n","from sklearn.metrics import confusion_matrix, recall_score, precision_score\n","from sklearn.preprocessing import binarize, PolynomialFeatures\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression, RidgeClassifier\n","from sklearn.calibration import calibration_curve\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","import warnings\n","from sklearn.metrics import mean_squared_error\n","warnings.filterwarnings('ignore')\n","%matplotlib inline\n","Creating functions which are being used repeateadly\n","# define a function to print accuracy metrics\n","def print_accuracy_metrics(Input,Output):\n","  print(\"Recall:\", recall_score(Input, Output))\n","  print(\"Log Loss:\", log_loss(Input, Output))\n","  print(\"Precision:\", precision_score(Input, Output))\n","  print(\"Accurcay:\", accuracy_score(Input, Output))\n","  print(\"AUC: \", roc_auc_score(Input, Output))\n","  print(\"F1 Score:\", f1_score(Input, Output))\n","  confusion_matrix_value = confusion_matrix(Input,Output)\n","  print('Confusion matrix:\\n', confusion_matrix_value)\n","  class_names=[0,1] # name  of classes\n","  fig, ax = plt.subplots()\n","  tick_marks = np.arange(len(class_names))\n","  plt.xticks(tick_marks, class_names)\n","  plt.yticks(tick_marks, class_names)\n","  # create heatmap\n","  sns.heatmap(pd.DataFrame( confusion_matrix_value), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n","  ax.xaxis.set_label_position(\"top\")\n","  plt.tight_layout()\n","  plt.title('Confusion matrix', y=1.1)\n","  plt.ylabel('Actual label')\n","  plt.xlabel('Predicted label')\n","  \n","# defined a function to print cross validation score\n","scoring = {'recall' : make_scorer(recall_score)}\n","def cross_validation_metrics(log_reg, X, y):\n"," log_reg_score = cross_val_score(log_reg, X,y,cv=5,scoring='recall')\n"," print('Logistic Regression Cross Validation Score(Recall): ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n","# function for plotting feature importance\n","def feature_importance(model, X):\n","  importances = model.feature_importances_\n","  std = np.std([tree.feature_importances_ for tree in model.estimators_],\n","               axis=0)\n","  indices = np.argsort(importances)[::-1]\n","\n","  # Print the feature ranking\n","  print(\"Feature ranking:\")\n","\n","  for f in range(X.shape[1]):\n","      print(\"%d. %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n","\n","  # Plot the feature importances of the forest\n","  plt.figure()\n","  plt.title(\"Feature importances\")\n","  plt.bar(range(X.shape[1]), importances[indices],\n","          color=\"r\", yerr=std[indices], align=\"center\")\n","  plt.xticks(range(X.shape[1]), indices)\n","  plt.xlim([-1, X.shape[1]])\n","  plt.show()\n","# function to draw ROC curve\n","def plot_auc_curve(model,):\n","  auc = roc_auc_score(y, y_pred_prob)\n","  fpr, tpr, thresholds = roc_curve(y, y_pred_prob)\n","  \n","  plt.plot(fpr, tpr)\n","  plt.xlim([0.0, 1.0])\n","  plt.ylim([0.0, 1.0])\n","  plt.title('ROC Curve\\n AUC={auc}'.format(auc = auc))\n","  plt.xlabel('False Positive Rate')\n","  plt.ylabel('True Positive Rate')\n","  plt.grid(True)\n","### Reading data as a pandas dataframe\n","data = pd.read_csv('creditcard.csv')\n","Data Exploration\n","#### Exploring data set\n","data.head()\n","id\tTime\tV1\tV2\tV3\tV4\tV5\tV6\tV7\tV8\t...\tV21\tV22\tV23\tV24\tV25\tV26\tV27\tV28\tAmount\tClass\n","0\t21749\t58670.0\t-0.854092\t0.644458\t1.805656\t1.146369\t-0.519127\t1.844676\t-0.935942\t1.056104\t...\t0.193673\t0.789467\t0.218834\t-0.577043\t-0.727521\t0.612977\t-0.219109\t-0.063157\t11.50\t0\n","1\t105607\t164361.0\t-0.863534\t0.291699\t0.594479\t-1.190707\t0.117851\t0.169880\t0.065587\t0.289947\t...\t-0.223345\t-0.333300\t-0.455269\t0.185385\t0.432974\t0.931127\t-0.414413\t-0.284978\t25.42\t0\n","2\t187884\t38767.0\t-1.192107\t-0.896044\t1.204410\t-1.593935\t0.432699\t-1.101769\t-0.299815\t0.222793\t...\t0.470749\t0.932440\t0.159099\t0.215700\t-0.169315\t-0.320951\t0.310243\t0.210535\t58.75\t0\n","3\t238501\t571.0\t-2.355336\t2.316182\t0.701735\t0.174501\t0.677346\t1.029705\t0.792868\t-0.060581\t...\t0.008872\t0.955806\t0.047292\t-0.650140\t-0.282282\t-0.286391\t0.335493\t0.223061\t2.89\t0\n","4\t252649\t51507.0\t-1.302336\t1.016359\t1.007046\t-0.127051\t0.435740\t-0.092143\t0.709650\t0.590142\t...\t0.150091\t0.059446\t-0.262177\t-0.354871\t0.680078\t-0.402172\t-0.111834\t-0.044427\t51.59\t0\n","5 rows × 32 columns\n","\n","data.describe()\n","id\tTime\tV1\tV2\tV3\tV4\tV5\tV6\tV7\tV8\t...\tV21\tV22\tV23\tV24\tV25\tV26\tV27\tV28\tAmount\tClass\n","count\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t...\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t244807.000000\t244807.000000\n","mean\t142394.376301\t94817.409077\t0.002267\t-0.002347\t0.001627\t0.000226\t0.000116\t-0.000695\t0.000235\t0.000451\t...\t-0.000663\t0.000085\t0.000678\t0.000009\t0.000082\t-0.000326\t0.000042\t0.000369\t88.221638\t0.001724\n","std\t82214.109334\t47544.660749\t1.958778\t1.655048\t1.511011\t1.414850\t1.378017\t1.332263\t1.229381\t1.174459\t...\t0.725261\t0.725420\t0.622169\t0.606201\t0.520588\t0.482409\t0.404819\t0.330222\t250.663749\t0.041483\n","min\t1.000000\t0.000000\t-56.407510\t-72.715728\t-48.325589\t-5.683171\t-113.743307\t-26.160506\t-37.060311\t-50.943369\t...\t-22.757540\t-10.933144\t-36.666000\t-2.836627\t-8.696627\t-2.604551\t-22.565679\t-15.430084\t0.000000\t0.000000\n","25%\t71174.500000\t54134.500000\t-0.918719\t-0.597789\t-0.888899\t-0.846298\t-0.690880\t-0.767694\t-0.553464\t-0.207827\t...\t-0.228840\t-0.543300\t-0.161045\t-0.354847\t-0.316905\t-0.327190\t-0.070876\t-0.052801\t5.610000\t0.000000\n","50%\t142272.000000\t84747.000000\t0.019709\t0.065045\t0.180200\t-0.020248\t-0.055758\t-0.273607\t0.039812\t0.022329\t...\t-0.029477\t0.006637\t-0.010750\t0.041093\t0.015969\t-0.052227\t0.001480\t0.011233\t22.000000\t0.000000\n","75%\t213591.500000\t139393.000000\t1.315616\t0.802233\t1.027866\t0.741737\t0.610432\t0.397342\t0.569658\t0.326222\t...\t0.186693\t0.530219\t0.147784\t0.439776\t0.350486\t0.240038\t0.091125\t0.078232\t77.050000\t0.000000\n","max\t284807.000000\t172792.000000\t2.454930\t19.167239\t9.382558\t16.875344\t34.801666\t73.301626\t120.589494\t20.007208\t...\t27.202839\t8.272233\t22.528412\t4.584549\t7.519589\t3.517346\t31.612198\t33.847808\t25691.160000\t1.000000\n","8 rows × 32 columns\n","\n","Observations\n","data.shape\n","(244807, 32)\n","data.columns\n","Index(['id', 'Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9',\n","       'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\n","       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n","       'Class'],\n","      dtype='object')\n","#### Checking for null values in dataset\n","data.isnull().sum().max()\n","0\n","#### There are no null values in dataset \n","####  Checking for unique values of ids\n","data.id.nunique()\n","244807\n","Data is pretty clean and there are no duplicate ids are present now let's check distribution of each feature\n","# Plot the histograms of each \n","data.hist(bins=50, figsize=(30,20))\n","plt.show()\n","\n","We can observe that all the features in dataset are scaled except amount and time.\n","So, in next step I am going to scale Amount column in dataset and delete time column.\n","data['normal_amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\n","data = data.drop(['Amount','Time'], axis=1)\n","X = data.loc[:,data.columns != 'Class']\n","y = data.loc[:,data.columns == 'Class']\n","f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n","\n","bins = 30\n","\n","ax1.hist(data.normal_amount[data.Class == 1], bins = bins)\n","ax1.set_title('Fraud')\n","\n","ax2.hist(data.normal_amount[data.Class == 0], bins = bins)\n","ax2.set_title('Normal')\n","\n","plt.xlabel('Amount ($)')\n","plt.ylabel('Number of Transactions')\n","plt.yscale('log')\n","plt.show()\n","\n","We can observe from the above figure that the fraud transactions amount is very less.\n","In next step I am going to visualize number of fraud transactions and number of Non-fraud transactions.\n","# Now lets check the class distributions\n","sns.countplot(\"Class\",data=data)\n","plt.title(\"Fraud class histogram\")\n","plt.xlabel(\"Class\")\n","plt.ylabel(\"Frequency\")\n","Text(0, 0.5, 'Frequency')\n","\n","As you can observe from the plot, we have so many 0 (non-fraud) compared to 1 (fraud).\n","This kind of imbalance in the target variable is known as class imbalance.\n","# Showing ratio\n","print(\"Percentage of normal transactions: \", len(data[data.Class == 0])/len(data))\n","print(\"Percentage of fraud transactions: \", len(data[data.Class == 1])/len(data))\n","print(\"Total number of transactions in data: \", len(data))\n","Percentage of normal transactions:  0.9982761930827141\n","Percentage of fraud transactions:  0.0017238069172858619\n","Total number of transactions in data:  244807\n","Most of the transactions were Non-Fraud (99.83%) of the time,\n","while Fraud transactions occurs 0.17% of the time in the dataframe.\n","Splitting original dataset into test and train\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n","1] Logistic regression on imbalanced dataset\n","lr = LogisticRegression()\n","lr.fit(X_train,y_train)\n","LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","          intercept_scaling=1, max_iter=100, multi_class='warn',\n","          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n","          tol=0.0001, verbose=0, warm_start=False)\n","# Accuracy metrics for \n","y_pred = lr.predict(X_test)\n","cross_validation_metrics(lr,X_train,y_train)\n","print_accuracy_metrics(y_test,y_pred)\n","Logistic Regression Cross Validation Score(Recall):  60.16%\n","Recall: 0.6481481481481481\n","Log Loss: 0.021162677654140833\n","Precision: 0.9090909090909091\n","Accurcay: 0.9993872799313753\n","AUC:  0.8240263478860329\n","F1 Score: 0.7567567567567568\n","Confusion matrix:\n"," [[73328     7]\n"," [   38    70]]\n","\n","Observatios\n","By observing the accuracy we can conclude that algorithm is performing extremely well . But it’s not true. As most of the labels 0, even random guess gives you 99% accuracy. So we need a better measure to understand the performance of the model.\n","\n","Recall\n","Recall is a measure which measures the ability of model to predict right for a given label. In our case, we want to test the model how accurately it can recall fraud cases as we are interested in that. As you can observe from the results, the recall for 1.0 is only 0.6016 compared to 99% for 0. So our model is not doing a good job of recognising frauds. So this shows that how imbalanced data is effecting accuracy of model.\n","\n","2] Using Class Weight (Logistic regression)\n","Scikit-learn logistic regression has a option named class_weight when specified does class imbalance handling implicitly. So trying to predict using this technique\n","\n","lr_balanced = LogisticRegression(class_weight = 'balanced')\n","lr_balanced.fit(X_train,y_train)\n","LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n","          fit_intercept=True, intercept_scaling=1, max_iter=100,\n","          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n","          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n","y_balanced_pred = lr_balanced.predict(X_test)\n","cross_validation_metrics(lr_balanced,X_train,y_train)\n","print_accuracy_metrics(y_test,y_balanced_pred)\n","Logistic Regression Cross Validation Score(Recall):  89.86%\n","Recall: 0.9115646258503401\n","Log Loss: 0.7882701074435376\n","Precision: 0.06470304200869145\n","Accurcay: 0.9771777676345634\n","AUC:  0.9444277359227315\n","F1 Score: 0.12082957619477007\n","Confusion matrix:\n"," [[83359  1937]\n"," [   13   134]]\n","\n","y_balanced_pred_prob = lr_balanced.predict_proba(X_test)[:, 1]\n","print('Prob:', y_balanced_pred_prob[0:20])\n","Prob: [0.11795312 0.10853555 0.16233188 0.04109433 0.06952336 0.8054823\n"," 0.33638875 0.01525269 0.01663033 0.45291477 0.03846819 0.02975751\n"," 0.00137332 0.00313013 0.00775784 0.05660248 0.01766819 0.00898342\n"," 0.09351198 0.05154421]\n","print('Prob:', y_balanced_pred[0:20])\n","Prob: [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","Undersampling of the dataset\n","Undersampling is one of the techniques used for handling class imbalance. In this technique, we under sample majority class to match the minority class. So in our example, we take random sample of non-fraud class to match number of fraud samples. This makes sure that the training data has equal amount of fraud and non-fraud samples.\n","\n","number_records_fraud = len(data[data.Class == 1])\n","fraud_indices = np.array(data[data.Class == 1].index)\n","normal_indices = data[data.Class == 0].index\n","random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n","random_normal_indices = np.array(random_normal_indices)\n","under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n","under_sample = data.iloc[under_sample_indices,:]\n","under_sample.shape\n","(844, 30)\n","So there are total 844 observations in our undersample dataframe.\n","\n","Visualising Undersampled Data\n","# Now lets check the class distributions\n","sns.countplot(\"Class\",data=under_sample)\n","plt.title(\"Fraud class histogram\")\n","plt.xlabel(\"Class\")\n","plt.ylabel(\"Frequency\")\n","Text(0, 0.5, 'Frequency')\n","\n","In the above plot, you can observe that classes are distributed evenly now.\n","If we try to correlate class and features on imbalanced dataset then it will be of no use because we will not see true correlations of features with result. While now I am going to see the features and their correlations w.r.t class on undersampled dataframe.\n","\n","# correlation matrix\n","corrmat =under_sample.corr()\n","fig,ax= plt.subplots()\n","fig.set_size_inches(25,15)\n","sns.heatmap(corrmat,square=True)\n","<matplotlib.axes._subplots.AxesSubplot at 0x1cea74d0>\n","\n","rf = RandomForestClassifier(n_estimators=100, \n","                            criterion='gini', \n","                            max_features='sqrt',\n","                            n_jobs=-1)\n","rf.fit(X_under_train, y_under_train)\n","RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n","            min_impurity_decrease=0.0, min_impurity_split=None,\n","            min_samples_leaf=1, min_samples_split=2,\n","            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n","            oob_score=False, random_state=None, verbose=0,\n","            warm_start=False)\n","feature_importance(rf,X_under_train)\n","Feature ranking:\n","1. V14 (0.163468)\n","2. V10 (0.142475)\n","3. V4 (0.135820)\n","4. V17 (0.077558)\n","5. V3 (0.066908)\n","6. V12 (0.063024)\n","7. V11 (0.059748)\n","8. V7 (0.035103)\n","9. V2 (0.029975)\n","10. V16 (0.025015)\n","11. V18 (0.020058)\n","12. V8 (0.014964)\n","13. V6 (0.014854)\n","14. V5 (0.011935)\n","15. V23 (0.011827)\n","16. V9 (0.011814)\n","17. V19 (0.011713)\n","18. V13 (0.011508)\n","19. V1 (0.010258)\n","20. V27 (0.010227)\n","21. V20 (0.010050)\n","22. normal_amount (0.009788)\n","23. V21 (0.008971)\n","24. V22 (0.008304)\n","25. V15 (0.008091)\n","26. V28 (0.006979)\n","27. V26 (0.006722)\n","28. V25 (0.006679)\n","29. V24 (0.006168)\n","\n","From the above heeatmap we can say that many of features are correlated but we are more interested in correlation of features with class. So I am going to list those features whose correlation coefficient w.r.t class is less than -0.6 or greater than 0.6\n","#negative correlations smaller than -0.5\n","corr = under_sample.corr()\n","corr = corr[['Class']]\n","corr[corr.Class < -0.6]\n","Class\n","V10\t-0.633606\n","V12\t-0.677653\n","V14\t-0.740321\n","#positive correlations greater than 0.5\n","corr[corr.Class > 0.6]\n","Class\n","V4\t0.698864\n","V11\t0.688469\n","Class\t1.000000\n","BoxPlots\n","We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions.\n","\n","#visualizing the features with high correlation\n","f, axes = plt.subplots(nrows=2, ncols=3, figsize=(26,16))\n","f.suptitle('Features With High Correlation', size=35)\n","sns.boxplot(x=\"Class\", y=\"V10\", data=under_sample, ax=axes[0,0])\n","sns.boxplot(x=\"Class\", y=\"V12\", data=under_sample, ax=axes[0,1])\n","sns.boxplot(x=\"Class\", y=\"V14\", data=under_sample, ax=axes[0,2])\n","sns.boxplot(x=\"Class\", y=\"V4\", data=under_sample, ax=axes[1,0])\n","sns.boxplot(x=\"Class\", y=\"V11\", data=under_sample, ax=axes[1,1])\n","f.delaxes(axes[1,2])\n","\n","Box plots provide us with a good intuition of whether we need to worry about outliers as all transactions outside of 1.5 times the IQR (Inter-Quartile Range) are usually considered to be outliers. However, removing all transactions outside of 1.5 times the IQR would dramatically decrease training data size, which is not very large, to begin with. Thus, I decided to only focus on extreme outliers outside of 2.5 times the IQR.\n","\n","under_sample.shape\n","(844, 30)\n","Removing extreme outliers\n","v14_fraud = under_sample['V14'].loc[under_sample['Class'] == 1].values\n","q25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\n","v14_iqr = q75 - q25\n","v14_cut_off = v14_iqr * 2.5\n","v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\n","under_sample= under_sample.drop(under_sample[(under_sample['V14'] > v14_upper) | (under_sample['V14'] < v14_lower)].index)\n","v12_fraud = under_sample['V12'].loc[under_sample['Class'] == 1].values\n","q25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\n","v12_iqr = q75 - q25\n","v12_cut_off = v12_iqr * 2.5\n","v12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\n","under_sample= under_sample.drop(under_sample[(under_sample['V12'] > v12_upper) | (under_sample['V12'] < v12_lower)].index)\n","v10_fraud = under_sample['V10'].loc[under_sample['Class'] == 1].values\n","q25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\n","v10_iqr = q75 - q25\n","v10_cut_off = v10_iqr * 2.5\n","v10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\n","under_sample= under_sample.drop(under_sample[(under_sample['V10'] > v10_upper) | (under_sample['V10'] < v10_lower)].index)\n","v4_fraud = under_sample['V4'].loc[under_sample['Class'] == 1].values\n","q25, q75 = np.percentile(v4_fraud, 25), np.percentile(v4_fraud, 75)\n","v4_iqr = q75 - q25\n","v4_cut_off = v4_iqr * 2.5\n","v4_lower, v4_upper = q25 - v4_cut_off, q75 + v4_cut_off\n","under_sample= under_sample.drop(under_sample[(under_sample['V4'] > v4_upper) | (under_sample['V4'] < v4_lower)].index)\n","v11_fraud = under_sample['V11'].loc[under_sample['Class'] == 1].values\n","q25, q75 = np.percentile(v11_fraud, 25), np.percentile(v11_fraud, 75)\n","v11_iqr = q75 - q25\n","v11_cut_off = v11_iqr * 2.5\n","v11_lower, v11_upper = q25 - v11_cut_off, q75 + v11_cut_off\n","under_sample= under_sample.drop(under_sample[(under_sample['V11'] > v11_upper) | (under_sample['V11'] < v11_lower)].index)\n","under_sample.shape\n","(976, 30)\n","After removing outliers our under sample dataframe is reduced to\n","I have tried to run this notebook without removing outliers and after removing outliers but I got better result before removing outliers.Before even thinking of removing outliers there should be enough evidence that these observations are actual outliers Before removing outliers. It should be done by in depth statistical analysis to make sure that these observations are actual outliers because different ML methods used for detecting fraud, are based on anomaly detection and they treat such extreme outliers as frauds. So, by deleting it we delete the most important observations, that have higher probability of being frauds\n","\n","Splitting under sampled dataframe\n","X_under = under_sample.loc[:,under_sample.columns != 'Class']\n","y_under = under_sample.loc[:,under_sample.columns == 'Class']\n","X_under_train, X_under_test, y_under_train, y_under_test = train_test_split(X_under,y_under,test_size = 0.3, random_state = 0)\n","3] Logistic regression with C=0.01\n","lr_under_C1 = LogisticRegression(C=0.01,penalty = 'l1')\n","lr_under_C1.fit(X_under_train,y_under_train)\n","LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n","          intercept_scaling=1, max_iter=100, multi_class='warn',\n","          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n","          tol=0.0001, verbose=0, warm_start=False)\n","# Prediction on original dataframe\n","y_pred_full_model1 = lr_under_C1.predict(X_test)\n","cross_validation_metrics(lr_under_C1,X_under_train,y_under_train)\n","print_accuracy_metrics(y_test,y_pred_full_model1)\n","Logistic Regression Cross Validation Score(Recall):  96.23%\n","Recall: 0.9115646258503401\n","Log Loss: 3.5007281778367507\n","Precision: 0.01526022093155677\n","Accurcay: 0.8986458808796508\n","AUC:  0.9050941212162975\n","F1 Score: 0.030017921146953407\n","Confusion matrix:\n"," [[76649  8647]\n"," [   13   134]]\n","\n","4] Logistic regression with C=0.1\n","lr_under_C2 = LogisticRegression(C=0.1,penalty = 'l1')\n","lr_under_C2.fit(X_under_train,y_under_train)\n","LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n","          intercept_scaling=1, max_iter=100, multi_class='warn',\n","          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n","          tol=0.0001, verbose=0, warm_start=False)\n","# Prediction on original dataset\n","y_pred_full_model2 = lr_under_C2.predict(X_test)\n","cross_validation_metrics(lr_under_C2,X_under_train,y_under_train)\n","print_accuracy_metrics(y_test,y_pred_full_model2)\n","Logistic Regression Cross Validation Score(Recall):  88.99%\n","Recall: 0.9115646258503401\n","Log Loss: 0.6209142741882784\n","Precision: 0.08086904043452021\n","Accurcay: 0.9820231031213792\n","AUC:  0.9468545789165413\n","F1 Score: 0.14855875831485585\n","Confusion matrix:\n"," [[83773  1523]\n"," [   13   134]]\n","\n","5] Logistic regression with C=1\n","lr_under_C3 = LogisticRegression(C=1,penalty = 'l1')\n","lr_under_C3.fit(X_under_train,y_under_train)\n","LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n","          intercept_scaling=1, max_iter=100, multi_class='warn',\n","          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n","          tol=0.0001, verbose=0, warm_start=False)\n","# Prediction on original dataset\n","y_pred_full_model3 = lr_under_C3.predict(X_test)\n","cross_validation_metrics(lr_under_C3,X_under_train,y_under_train)\n","print_accuracy_metrics(y_test,y_pred_full_model3)\n","Logistic Regression Cross Validation Score(Recall):  91.19%\n","Recall: 0.9259259259259259\n","Log Loss: 1.4306248896684717\n","Precision: 0.03190810465858328\n","Accurcay: 0.9585801233609739\n","AUC:  0.9422770694605426\n","F1 Score: 0.061690314620604564\n","Confusion matrix:\n"," [[70301  3034]\n"," [    8   100]]\n","\n","6] Logistic regreesion with C=10\n","lr_under_C4 = LogisticRegression(C=10,penalty = 'l1')\n","lr_under_C4.fit(X_under_train,y_under_train)\n","LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n","          intercept_scaling=1, max_iter=100, multi_class='warn',\n","          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n","          tol=0.0001, verbose=0, warm_start=False)\n","# Prediction on original dataset\n","y_pred_full_model4 = lr_under_C4.predict(X_test)\n","cross_validation_metrics(lr_under_C4,X_under_train,y_under_train)\n","print_accuracy_metrics(y_test,y_pred_full_model4)\n","Logistic Regression Cross Validation Score(Recall):  90.51%\n","Recall: 0.9259259259259259\n","Log Loss: 1.6827008224584545\n","Precision: 0.027247956403269755\n","Accurcay: 0.951281946543578\n","AUC:  0.9386226070619608\n","F1 Score: 0.05293806246691372\n","Confusion matrix:\n"," [[69765  3570]\n"," [    8   100]]\n","\n","7] Decision Tree Classifier\n","DecisionTreeClassifier= DecisionTreeClassifier()\n","DecisionTreeClassifier.fit(X_under_train,y_under_train)\n","DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n","            max_features=None, max_leaf_nodes=None,\n","            min_impurity_decrease=0.0, min_impurity_split=None,\n","            min_samples_leaf=1, min_samples_split=2,\n","            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n","            splitter='best')\n","# Prediction on original dataset\n","y_pred_DecisionTree = DecisionTreeClassifier.predict(X_test)\n","cross_validation_metrics(DecisionTreeClassifier,X_under_train,y_under_train)\n","print_accuracy_metrics(y_test,y_pred_DecisionTree)\n","Logistic Regression Cross Validation Score(Recall):  87.12%\n","Recall: 0.9722222222222222\n","Log Loss: 3.9090580892603124\n","Precision: 0.012479201331114808\n","Accurcay: 0.886823795324265\n","AUC:  0.9294601259062294\n","F1 Score: 0.02464210279277165\n","Confusion matrix:\n"," [[65026  8309]\n"," [    3   105]]\n","\n","plt.figure(0).clf()\n","\n","fpr, tpr, thresh = metrics.roc_curve(y_test, y_balanced_pred)\n","auc = metrics.roc_auc_score(y_test, y_balanced_pred)\n","plt.plot(fpr,tpr,label=\"Logistic Regtession Class weight, auc=\"+ '{0:.3f}'.format(auc))\n","\n","fpr, tpr, thresh = metrics.roc_curve(y_test, y_pred_full_model1)\n","auc = metrics.roc_auc_score(y_test, y_pred_full_model1)\n","plt.plot(fpr,tpr,label=\"Logistic regression(C=0.01), auc=\"+ '{0:.3f}'.format(auc))\n","\n","fpr, tpr, thresh = metrics.roc_curve(y_test, y_pred_full_model2)\n","auc = metrics.roc_auc_score(y_test, y_pred_full_model2)\n","plt.plot(fpr,tpr,label=\"Logistic regression(C=0.1), auc=\"+'{0:.3f}'.format(auc))\n","\n","fpr, tpr, thresh = metrics.roc_curve(y_test, y_pred_full_model3)\n","auc = metrics.roc_auc_score(y_test, y_pred_full_model3)\n","plt.plot(fpr,tpr,label=\"Logistic regression(C=1), auc=\"+'{0:.3f}'.format(auc))\n","\n","fpr, tpr, thresh = metrics.roc_curve(y_test, y_pred_full_model4)\n","auc = metrics.roc_auc_score(y_test, y_pred_full_model4)\n","plt.plot(fpr,tpr,label=\"Logistic regression(C=10), auc=\"+'{0:.3f}'.format(auc))\n","\n","plt.plot(fpr, tpr)\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.0])\n","plt.title('ROC Curve\\n AUC={auc}'.format(auc = auc))\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.grid(True)\n","plt.legend(loc=\"lower right\")\n","<matplotlib.legend.Legend at 0x2293b290>\n","\n","Obviously, trying to increase recall, tends to come with a decrease of precision. However, in our case, if we predict that a transaction is fraudulent and turns out not to be, is not a massive problem compared to the opposite.\n","\n","Predictions on new samples\n","data_test=pd.read_csv('creditcard_test_dataset.csv')\n","data_test.head()\n","id\tTime\tV1\tV2\tV3\tV4\tV5\tV6\tV7\tV8\t...\tV20\tV21\tV22\tV23\tV24\tV25\tV26\tV27\tV28\tAmount\n","0\t30808\t109847\t1.930307\t-0.234417\t-1.583368\t0.024786\t1.016470\t1.179664\t-0.386242\t0.327416\t...\t-0.437585\t0.211402\t1.015098\t0.073241\t-0.804496\t0.072202\t-0.383211\t0.016787\t-0.081815\t1.00\n","1\t174948\t84730\t-5.053316\t-3.617236\t-0.323455\t1.447171\t-0.097495\t-0.154917\t-2.076441\t1.331307\t...\t-1.351008\t-0.202483\t0.036442\t-1.901898\t0.090641\t-1.777126\t0.947972\t0.954071\t-1.577919\t333.48\n","2\t203982\t95957\t0.090684\t1.197902\t-1.370219\t0.770437\t0.857203\t-0.698486\t1.350617\t-0.952780\t...\t0.617074\t-0.255027\t0.378177\t-0.126596\t0.572250\t0.347829\t0.538705\t-0.211582\t-0.267241\t50.14\n","3\t283146\t142109\t1.808668\t-1.197522\t-2.291089\t-0.690236\t-0.312654\t-1.756994\t0.631548\t-0.725965\t...\t0.457894\t0.498576\t0.927097\t-0.283918\t0.134011\t0.372327\t0.050185\t-0.121994\t-0.038088\t254.04\n","4\t47316\t129034\t-0.920372\t1.003867\t-0.110709\t-2.771173\t0.841827\t-0.497278\t0.769851\t0.474075\t...\t-0.175248\t-0.238352\t-0.849203\t-0.124786\t0.086646\t-0.007944\t0.360582\t0.090110\t0.091000\t1.00\n","5 rows × 31 columns\n","\n","data_test.tail()\n","id\tTime\tV1\tV2\tV3\tV4\tV5\tV6\tV7\tV8\t...\tV20\tV21\tV22\tV23\tV24\tV25\tV26\tV27\tV28\tAmount\n","39995\t33211\t87180\t-0.227015\t1.066681\t0.167350\t0.847756\t0.918577\t-0.569415\t1.412774\t-0.317352\t...\t0.007818\t0.115365\t0.685676\t-0.300523\t-0.008611\t-0.105765\t-0.433028\t0.129265\t0.010438\t13.99\n","39996\t226678\t66735\t1.080631\t0.072792\t0.193645\t1.189376\t-0.379073\t-0.941658\t0.310233\t-0.237765\t...\t0.009391\t0.075056\t0.056154\t-0.170659\t0.388427\t0.646854\t-0.336122\t-0.004275\t0.034426\t89.16\n","39997\t91144\t69391\t-0.366346\t-0.123654\t1.574127\t-1.620753\t-1.152640\t-0.860367\t-0.407476\t-0.123524\t...\t-0.106231\t-0.167896\t-0.034830\t-0.125103\t0.357849\t-0.137647\t-0.134737\t0.253740\t0.180461\t10.00\n","39998\t151948\t84607\t1.245859\t-0.208636\t-0.083039\t-0.792677\t-0.621157\t-1.219052\t0.085329\t-0.130222\t...\t-0.217895\t0.033446\t0.241614\t-0.169362\t0.550034\t0.853734\t-0.576327\t0.022538\t0.001399\t7.71\n","39999\t80174\t70403\t-0.633590\t0.812096\t2.561737\t0.446063\t-0.448034\t-0.683693\t0.267501\t-0.008028\t...\t0.149068\t0.007932\t-0.015742\t-0.131304\t0.736886\t-0.036651\t0.325312\t0.051134\t0.087213\t6.47\n","5 rows × 31 columns\n","\n","data_test['normal_amount'] = StandardScaler().fit_transform(data_test['Amount'].values.reshape(-1,1))\n","data_test1 = data_test.drop(['Amount','Time','id'], axis=1)\n","new_pred_class = lr_under_C2.predict(data_test1)\n","new_pred_class\n","array([0, 0, 0, ..., 0, 0, 0], dtype=int64)\n","new_pred_class_prob = lr_balanced.predict_proba(data_test1)[:, 1]\n","new_pred_class_prob\n","array([0.04646066, 0.00050197, 0.05586541, ..., 0.01563743, 0.03944736,\n","       0.06019283])\n","pd.DataFrame({'ID':data_test.id,'Class':new_pred_class}).set_index('ID').to_csv('submission_credit1.csv')\n","pd.DataFrame({'ID':data_test.id,'Class':new_pred_class_prob}).set_index('ID').to_csv('submission_credit_prob1.csv')\n"]}]}